Wpływ rozmiaru warstwy ukrytej na metryki modelu mlp na zbiorze mnist
---------------------------------------------------------------------
Tabela (oryginalna kolejność):
hidden | train_loss | val_loss | val_acc | val_acc_%
----------------------------------------------------
20     | 0.089359   | 0.136123 | 0.9633  | 96.33%   
50     | 0.026360   | 0.085119 | 0.9762  | 97.62%   
75     | 0.007003   | 0.086732 | 0.9783  | 97.83%   
90     | 0.005079   | 0.085532 | 0.9788  | 97.88%   
100    | 0.012881   | 0.070083 | 0.9807  | 98.07%   
110    | 0.001794   | 0.090969 | 0.9801  | 98.01%   
130    | 0.004470   | 0.076544 | 0.9800  | 98.00%   
150    | 0.004356   | 0.078532 | 0.9795  | 97.95%   
175    | 0.001928   | 0.070662 | 0.9822  | 98.22%   
200    | 0.000097   | 0.109160 | 0.9798  | 97.98%   
250    | 0.001156   | 0.074830 | 0.9824  | 98.24%   
300    | 0.000031   | 0.105100 | 0.9826  | 98.26%   
400    | 0.000191   | 0.078243 | 0.9835  | 98.35%   
500    | 0.000494   | 0.070795 | 0.9839  | 98.39%   
750    | 0.000032   | 0.086414 | 0.9835  | 98.35%   
1000   | 0.000046   | 0.085519 | 0.9839  | 98.39%   
1500   | 0.000077   | 0.078888 | 0.9844  | 98.44%   
2000   | 0.000147   | 0.074267 | 0.9834  | 98.34%   
3000   | 0.000440   | 0.063430 | 0.9845  | 98.45%   
5000   | 0.001590   | 0.059876 | 0.9838  | 98.38%   

Tabela (posortowana po val_acc ↓):
hidden | train_loss | val_loss | val_acc | val_acc_%
----------------------------------------------------
3000   | 0.000440   | 0.063430 | 0.9845  | 98.45%   
1500   | 0.000077   | 0.078888 | 0.9844  | 98.44%   
500    | 0.000494   | 0.070795 | 0.9839  | 98.39%   
1000   | 0.000046   | 0.085519 | 0.9839  | 98.39%   
5000   | 0.001590   | 0.059876 | 0.9838  | 98.38%   
400    | 0.000191   | 0.078243 | 0.9835  | 98.35%   
750    | 0.000032   | 0.086414 | 0.9835  | 98.35%   
2000   | 0.000147   | 0.074267 | 0.9834  | 98.34%   
300    | 0.000031   | 0.105100 | 0.9826  | 98.26%   
250    | 0.001156   | 0.074830 | 0.9824  | 98.24%   
175    | 0.001928   | 0.070662 | 0.9822  | 98.22%   
100    | 0.012881   | 0.070083 | 0.9807  | 98.07%   
110    | 0.001794   | 0.090969 | 0.9801  | 98.01%   
130    | 0.004470   | 0.076544 | 0.9800  | 98.00%   
200    | 0.000097   | 0.109160 | 0.9798  | 97.98%   
150    | 0.004356   | 0.078532 | 0.9795  | 97.95%   
90     | 0.005079   | 0.085532 | 0.9788  | 97.88%   
75     | 0.007003   | 0.086732 | 0.9783  | 97.83%   
50     | 0.026360   | 0.085119 | 0.9762  | 97.62%   
20     | 0.089359   | 0.136123 | 0.9633  | 96.33%   

Tabela (posortowana po val_loss ↓):
hidden | train_loss | val_loss | val_acc | val_acc_%
----------------------------------------------------
20     | 0.089359   | 0.136123 | 0.9633  | 96.33%   
200    | 0.000097   | 0.109160 | 0.9798  | 97.98%   
300    | 0.000031   | 0.105100 | 0.9826  | 98.26%   
110    | 0.001794   | 0.090969 | 0.9801  | 98.01%   
75     | 0.007003   | 0.086732 | 0.9783  | 97.83%   
750    | 0.000032   | 0.086414 | 0.9835  | 98.35%   
90     | 0.005079   | 0.085532 | 0.9788  | 97.88%   
1000   | 0.000046   | 0.085519 | 0.9839  | 98.39%   
50     | 0.026360   | 0.085119 | 0.9762  | 97.62%   
1500   | 0.000077   | 0.078888 | 0.9844  | 98.44%   
150    | 0.004356   | 0.078532 | 0.9795  | 97.95%   
400    | 0.000191   | 0.078243 | 0.9835  | 98.35%   
130    | 0.004470   | 0.076544 | 0.9800  | 98.00%   
250    | 0.001156   | 0.074830 | 0.9824  | 98.24%   
2000   | 0.000147   | 0.074267 | 0.9834  | 98.34%   
500    | 0.000494   | 0.070795 | 0.9839  | 98.39%   
175    | 0.001928   | 0.070662 | 0.9822  | 98.22%   
100    | 0.012881   | 0.070083 | 0.9807  | 98.07%   
3000   | 0.000440   | 0.063430 | 0.9845  | 98.45%   
5000   | 0.001590   | 0.059876 | 0.9838  | 98.38%   